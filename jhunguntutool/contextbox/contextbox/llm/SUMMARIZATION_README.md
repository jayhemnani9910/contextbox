# Intelligent Summarization System for ContextBox

A comprehensive, production-ready summarization system with map-reduce functionality, multiple LLM backend support, and advanced features for intelligent content processing.

## Table of Contents

1. [Overview and Features](#overview-and-features)
2. [Quick Start Guide](#quick-start-guide)
3. [API Reference](#api-reference)
4. [Configuration Guide](#configuration-guide)
5. [Map-Reduce Process](#map-reduce-process)
6. [Content Type Specifications](#content-type-specifications)
7. [Quality Metrics](#quality-metrics)
8. [Caching Behavior](#caching-behavior)
9. [Export Formats](#export-formats)
10. [Database Integration](#database-integration)
11. [Error Handling](#error-handling)
12. [Performance Optimization](#performance-optimization)

## Overview and Features

### Core Features

The Intelligent Summarization System provides enterprise-grade summarization capabilities:

- **ðŸ—ºï¸ Map-Reduce Processing**: Handles large documents through intelligent chunking and parallel processing
- **ðŸ“ Multiple Summary Types**: Brief, detailed, and executive summaries with customizable formats
- **ðŸŽ¯ Content Type Awareness**: Specialized handling for articles, transcripts, documentation, code, and news
- **âš¡ Progressive Summarization**: Quick overview â†’ detailed expansion on demand
- **ðŸ“Š Quality Assessment**: Multi-dimensional quality scoring with automatic validation
- **ðŸ’¾ Intelligent Caching**: LRU caching system with SQLite backend for performance
- **ðŸ”„ Multi-Document Support**: Comparative and synthesis summarization across documents
- **ðŸš€ LLM Backend Flexibility**: Support for Ollama, OpenAI, and custom backends
- **ðŸ’¾ Database Integration**: Seamless integration with ContextBox database
- **ðŸ“¤ Export Capabilities**: JSON, Markdown, and plain text export formats

### Key Benefits

- **Scalable**: Process documents of any size through map-reduce architecture
- **Flexible**: Multiple summary lengths and formats for different use cases
- **Quality-Assured**: Built-in quality metrics and validation
- **Performance-Optimized**: Intelligent caching and chunking strategies
- **Production-Ready**: Comprehensive error handling and monitoring

## Quick Start Guide

### Basic Usage

```python
from contextbox.llm.summarization import (
    SummarizationManager, 
    SummaryRequest,
    create_summarization_manager
)

# Method 1: Create manager with default settings
manager = create_summarization_manager()

# Method 2: Create with custom configuration
from contextbox.llm.summarization import SummarizationManager

manager = SummarizationManager(
    config_path="path/to/llm_config.json",
    cache_db_path="summaries_cache.db"
)

# Basic summarization
request = SummaryRequest(
    content="Your content to summarize here...",
    content_type="article",
    summary_length="detailed",
    format_type="paragraph"
)

result = manager.summarize_content(request)
print(result.summary.text)
print(f"Quality score: {result.quality_metrics['overall']}")
```

### Real-World Examples

#### 1. Article Summarization

```python
# Summarize a news article
article_content = """
Latest breakthrough in quantum computing demonstrates significant progress 
in error correction algorithms. Research team at MIT successfully implemented 
a 99.9% accuracy rate in quantum operations using novel error correction 
protocols. This advancement brings practical quantum computing applications 
closer to reality, with potential applications in cryptography, drug discovery, 
and financial modeling. The technology is expected to be commercially available 
within the next 5-7 years.
"""

request = SummaryRequest(
    content=article_content,
    content_type="article",
    summary_length="brief",
    format_type="bullets",
    quality_threshold=0.8
)

result = manager.summarize_content(request)
print(result.summary.text)
```

#### 2. Progressive Summarization

```python
# Get quick overview, expandable to detailed
request = SummaryRequest(
    content=long_document,
    content_type="article",
    summary_length="detailed",
    enable_progressive=True,
    format_type="paragraph"
)

result = manager.summarize_content(request)
# Returns brief summary with option to expand
print(result.summary.text)
```

#### 3. Multi-Document Analysis

```python
# Summarize multiple documents together
documents = [
    (transcript1, "transcript"),
    (transcript2, "transcript"),
    (article1, "article")
]

multi_result = manager.summarize_multiple_documents(
    documents=documents,
    summary_type="comparative",
    summary_length="detailed"
)

print(f"Combined Summary: {multi_result.combined_summary.text}")
print(f"Common Themes: {multi_result.common_themes}")
print(f"Cross-Document Insights: {multi_result.cross_document_insights}")
```

#### 4. Export Results

```python
# Export to different formats
manager.export_summary(result, "summary.json", "json")
manager.export_summary(result, "summary.md", "markdown")
manager.export_summary(result, "summary.txt", "text")
```

## API Reference

### SummarizationManager

The main orchestrator class for the summarization system.

#### Constructor

```python
SummarizationManager(
    config_path: Optional[str] = None,
    cache_db_path: str = "contextbox_summaries.db",
    logger: Optional[logging.Logger] = None
)
```

**Parameters:**
- `config_path`: Path to LLM configuration file
- `cache_db_path`: Path to SQLite cache database
- `logger`: Optional logger instance

#### Methods

##### `summarize_content(request: SummaryRequest) -> SummaryResult`

Main method for summarizing content.

**Parameters:**
- `request`: SummaryRequest containing content and parameters

**Returns:**
- SummaryResult with generated summary and metadata

##### `summarize_multiple_documents(documents, summary_type="comparative", **kwargs) -> MultiDocumentSummary`

Summarize multiple documents together.

**Parameters:**
- `documents`: List of (content, content_type) tuples
- `summary_type`: Type of multi-document summary ("comparative", "synthesis")
- `**kwargs`: Additional summarization parameters

**Returns:**
- MultiDocumentSummary with combined insights

##### `export_summary(summary_result, output_path, format_type="json")`

Export summary result to file.

**Parameters:**
- `summary_result`: SummaryResult to export
- `output_path`: Output file path
- `format_type`: Export format ("json", "markdown", "text")

##### `get_cache_stats() -> Dict[str, Any]`

Get cache statistics.

**Returns:**
- Dictionary containing cache statistics

##### `clear_cache(older_than_days=30)`

Clear old cache entries.

**Parameters:**
- `older_than_days`: Remove entries older than specified days

##### `health_check() -> Dict[str, Any]`

Perform health check on the system.

**Returns:**
- Dictionary with system health status

### Data Classes

#### SummaryRequest

```python
@dataclass
class SummaryRequest:
    content: str
    content_type: str
    source_id: Optional[str] = None
    summary_length: str = "detailed"  # brief, detailed, executive
    format_type: str = "paragraph"    # paragraph, bullets, key_points
    include_metadata: bool = True
    quality_threshold: float = 0.7
    enable_progressive: bool = True
    enable_caching: bool = True
    max_retries: int = 3
    timeout: int = 30
```

#### SummaryResult

```python
@dataclass
class SummaryResult:
    summary: SummaryContent
    quality_metrics: Dict[str, float]
    processing_info: Dict[str, Any]
    cache_hit: bool = False
    llm_provider: Optional[str] = None
    model_used: Optional[str] = None
    error: Optional[str] = None
```

#### SummaryContent

```python
@dataclass
class SummaryContent:
    text: str
    content_type: str
    source_id: Optional[str] = None
    chunk_index: Optional[int] = None
    chunk_count: Optional[int] = None
    processing_time: Optional[float] = None
    token_count: Optional[int] = None
    quality_score: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None
```

## Configuration Guide

### LLM Backend Configuration

Create a configuration file (`llm_config.json`) to configure LLM backends:

```json
{
  "providers": {
    "ollama": {
      "api_key": null,
      "base_url": "http://localhost:11434",
      "models": {
        "llama2": {
          "model_type": "chat",
          "max_tokens": 4096,
          "temperature": 0.7,
          "top_p": 1.0
        },
        "codellama": {
          "model_type": "chat",
          "max_tokens": 4096,
          "temperature": 0.3,
          "top_p": 0.9
        }
      },
      "default_model": "llama2",
      "timeout": 30,
      "max_retries": 3
    },
    "openai": {
      "api_key": "your-api-key-here",
      "models": {
        "gpt-4": {
          "model_type": "chat",
          "max_tokens": 4096,
          "temperature": 0.7,
          "top_p": 1.0,
          "cost_per_input_token": 0.00003,
          "cost_per_output_token": 0.00006
        },
        "gpt-3.5-turbo": {
          "model_type": "chat",
          "max_tokens": 4096,
          "temperature": 0.7,
          "top_p": 1.0,
          "cost_per_input_token": 0.0015,
          "cost_per_output_token": 0.002
        }
      },
      "default_model": "gpt-3.5-turbo",
      "timeout": 60,
      "max_retries": 3,
      "rate_limit": {
        "requests_per_minute": 60,
        "requests_per_hour": 1000,
        "tokens_per_minute": 100000
      },
      "cost_config": {
        "track_usage": true,
        "track_costs": true,
        "budget_limit": 100.0
      }
    }
  },
  "default_provider": "openai",
  "enable_logging": true,
  "log_level": "INFO",
  "enable_monitoring": true
}
```

### Ollama Setup

1. Install Ollama: https://ollama.ai
2. Pull models: `ollama pull llama2`, `ollama pull codellama`
3. Configure the configuration file with Ollama settings
4. Start the summarization system

### OpenAI Setup

1. Get API key from OpenAI platform
2. Add API key to configuration file
3. Configure models and rate limits
4. The system will automatically use OpenAI backend

### Mock Backend for Testing

For development and testing without API access:

```python
from contextbox.llm.mock_backend import MockLLMBackend

manager = SummarizationManager()
mock_backend = MockLLMBackend()
manager.backends['mock'] = mock_backend
manager.config.default_provider = 'mock'
```

## Map-Reduce Process

The system implements a sophisticated map-reduce architecture for processing large documents:

### Map Phase

1. **Content Chunking**: Intelligent segmentation based on content type
   - **Articles**: Chunk by headings and sections
   - **Transcripts**: Chunk by speaker changes and timestamps
   - **Code**: Chunk by functions and classes
   - **General Text**: Chunk by paragraphs with overlap preservation

2. **Parallel Processing**: Each chunk is processed independently
   - Chunk-specific prompts for optimal results
   - Content-type aware summarization strategies
   - Quality assessment for each chunk summary

### Reduce Phase

1. **Summary Combination**: Merge chunk summaries into coherent result
   - Remove redundancy and contradictions
   - Maintain logical flow and structure
   - Preserve important details from all chunks

2. **Final Synthesis**: Generate comprehensive summary
   - Cross-chunk consistency checks
   - Overall quality assessment
   - Metadata preservation

### Example Map-Reduce Workflow

```python
# Large document processing
request = SummaryRequest(
    content=large_document,
    content_type="article",
    summary_length="detailed"
)

# Automatically uses map-reduce for documents > 2000 characters
result = manager.summarize_content(request)

print(f"Processed {result.processing_info['chunk_count']} chunks")
print(f"Map errors: {result.processing_info.get('map_errors', [])}")
```

## Content Type Specifications

The system provides specialized handling for different content types:

### Article
- **Characteristics**: Structured text with headings, sections, conclusions
- **Chunking Strategy**: By headings and logical sections
- **Prompt Focus**: Main arguments, evidence, conclusions
- **Quality Metrics**: Emphasis on coherence and completeness

```python
# Article summary example
request = SummaryRequest(
    content=blog_post,
    content_type="article",
    summary_length="detailed",
    format_type="bullets"
)
```

### Transcript
- **Characteristics**: Spoken content with speakers, timestamps
- **Chunking Strategy**: By speaker changes and natural breaks
- **Prompt Focus**: Key speakers, main topics, important quotes
- **Quality Metrics**: Emphasis on completeness and relevance

```python
# Transcript summary example
request = SummaryRequest(
    content=meeting_transcript,
    content_type="transcript",
    summary_length="executive",
    format_type="key_points"
)
```

### Documentation
- **Characteristics**: Technical content with setup, usage instructions
- **Chunking Strategy**: By code blocks and structural elements
- **Prompt Focus**: Functionality, setup, usage instructions
- **Quality Metrics**: Emphasis on completeness and accuracy

```python
# Documentation summary example
request = SummaryRequest(
    content=api_docs,
    content_type="documentation",
    summary_length="detailed",
    format_type="paragraph"
)
```

### Code
- **Characteristics**: Source code with functions, classes, logic
- **Chunking Strategy**: By functions, classes, and logical blocks
- **Prompt Focus**: Purpose, main functions, implementation details
- **Quality Metrics**: Emphasis on technical accuracy

```python
# Code summary example
request = SummaryRequest(
    content=source_code,
    content_type="code",
    summary_length="brief",
    format_type="key_points"
)
```

### News
- **Characteristics**: Factual reporting with 5W+H structure
- **Chunking Strategy**: By paragraphs and logical flow
- **Prompt Focus**: Who, what, when, where, why, how
- **Quality Metrics**: Emphasis on completeness and factual accuracy

## Quality Metrics

The system provides comprehensive quality assessment across multiple dimensions:

### Quality Dimensions

#### 1. Coherence (30% weight)
- **Assessment**: Logical flow and connecting elements
- **Metrics**: Presence of connecting words, sentence length consistency
- **Range**: 0.0 - 1.0
- **Example**: "However, therefore, furthermore" usage frequency

#### 2. Completeness (30% weight)
- **Assessment**: Coverage of original content concepts
- **Metrics**: Concept overlap between original and summary
- **Range**: 0.0 - 1.0
- **Calculation**: (Summary concepts âˆ© Original concepts) / Original concepts

#### 3. Conciseness (20% weight)
- **Assessment**: Appropriate length for requested summary type
- **Metrics**: Word count vs. expected range
- **Range**: 0.0 - 1.0
- **Expected Ranges**:
  - Brief: 50-150 words
  - Detailed: 150-400 words
  - Executive: 100-250 words

#### 4. Relevance (20% weight)
- **Assessment**: Alignment with original content
- **Metrics**: Key term overlap, proper noun consistency
- **Range**: 0.0 - 1.0
- **Calculation**: Summary key terms / Original key terms

### Overall Quality Score

```python
overall_score = (
    coherence * 0.3 +
    completeness * 0.3 +
    conciseness * 0.2 +
    relevance * 0.2
)
```

### Quality Threshold Handling

```python
# Set quality threshold
request = SummaryRequest(
    content=content,
    quality_threshold=0.8  # Minimum acceptable quality
)

result = manager.summarize_content(request)

# Check quality
if result.quality_metrics['overall'] >= request.quality_threshold:
    print("Quality acceptable")
else:
    print("Quality below threshold, consider retrying")
```

### Custom Quality Assessment

```python
# Access individual metrics
print(f"Coherence: {result.quality_metrics['coherence']:.2f}")
print(f"Completeness: {result.quality_metrics['completeness']:.2f}")
print(f"Conciseness: {result.quality_metrics['conciseness']:.2f}")
print(f"Relevance: {result.quality_metrics['relevance']:.2f}")
print(f"Overall: {result.quality_metrics['overall']:.2f}")
```

## Caching Behavior

The system implements intelligent caching for optimal performance:

### Cache Architecture

- **Backend**: SQLite database for persistence
- **Strategy**: Content hash + request parameters
- **Eviction**: LRU (Least Recently Used)
- **Storage**: Full summary + metadata + quality scores

### Cache Key Generation

```python
def _generate_cache_key(self, content: str, request: SummaryRequest) -> str:
    content_hash = hashlib.sha256(content.encode()).hexdigest()
    
    key_data = {
        'content_hash': content_hash,
        'content_type': request.content_type,
        'summary_length': request.summary_length,
        'format_type': request.format_type,
        'include_metadata': request.include_metadata
    }
    
    return hashlib.sha256(
        json.dumps(key_data, sort_keys=True).encode()
    ).hexdigest()
```

### Cache Statistics

```python
# Get cache performance metrics
stats = manager.get_cache_stats()
print(f"Total entries: {stats['total_entries']}")
print(f"Average access count: {stats['avg_access_count']:.2f}")
print(f"Oldest entry: {stats['oldest_entry']}")
print(f"Most recent access: {stats['most_recent_access']}")
```

### Cache Configuration

```python
# Disable caching for specific request
request = SummaryRequest(
    content=content,
    enable_caching=False  # Skip cache
)

# Enable caching (default)
request = SummaryRequest(
    content=content,
    enable_caching=True  # Use cache
)
```

### Cache Management

```python
# Clear old cache entries
manager.clear_cache(older_than_days=30)  # Remove entries >30 days old

# Cache is automatically updated during summarization
result = manager.summarize_content(request)
if result.cache_hit:
    print("Retrieved from cache")
else:
    print("Generated new summary (cached)")
```

### Performance Benefits

- **Speed**: Instant retrieval for repeated content
- **Efficiency**: Reduced LLM API calls
- **Consistency**: Same parameters always return same result
- **Analytics**: Track summary usage patterns

## Export Formats

The system supports multiple export formats for different use cases:

### JSON Format

Comprehensive export with full metadata:

```python
manager.export_summary(result, "summary.json", "json")
```

**JSON Structure:**
```json
{
  "summary": {
    "text": "Summary content here...",
    "content_type": "article",
    "source_id": "source_123",
    "metadata": {
      "chunk_count": 3,
      "processing_time": 2.45,
      "map_reduce": true
    }
  },
  "quality_metrics": {
    "coherence": 0.85,
    "completeness": 0.92,
    "conciseness": 0.78,
    "relevance": 0.88,
    "overall": 0.86
  },
  "processing_info": {
    "timestamp": "2025-01-15T10:30:00Z",
    "chunk_count": 3,
    "include_metadata": true,
    "llm_info": {
      "provider": "openai",
      "model": "gpt-3.5-turbo",
      "total_tokens": 2845
    }
  },
  "export_timestamp": "2025-01-15T10:30:15Z"
}
```

### Markdown Format

Human-readable with structured sections:

```python
manager.export_summary(result, "summary.md", "markdown")
```

**Markdown Structure:**
```markdown
# Summary

**Content Type:** article  
**Source ID:** source_123  
**Generated:** 2025-01-15T10:30:00Z

## Quality Metrics

- **Coherence:** 0.85
- **Completeness:** 0.92
- **Conciseness:** 0.78
- **Relevance:** 0.88

## Summary

Summary content here...

## Metadata

```json
{
  "chunk_count": 3,
  "processing_time": 2.45,
  "map_reduce": true
}
```
```

### Plain Text Format

Simple, readable format:

```python
manager.export_summary(result, "summary.txt", "text")
```

**Plain Text Structure:**
```

==================================================
SUMMARY
==================================================

Content Type: article
Source ID: source_123
Generated: 2025-01-15T10:30:00Z

QUALITY METRICS:
  Coherence: 0.85
  Completeness: 0.92
  Conciseness: 0.78
  Relevance: 0.88

--------------------------------------------------

Summary content here...

==================================================
```

### Custom Export

```python
# Get structured data for custom processing
export_data = {
    'summary_text': result.summary.text,
    'quality_score': result.quality_metrics['overall'],
    'processing_time': result.summary.processing_time,
    'word_count': len(result.summary.text.split()),
    'chunk_count': result.processing_info.get('chunk_count', 1)
}

# Save to custom format
import json
with open("custom_summary.json", "w") as f:
    json.dump(export_data, f, indent=2)
```

## Database Integration

The system seamlessly integrates with ContextBox database:

### DatabaseIntegratedSummarizer

```python
from contextbox.llm.summarization import DatabaseIntegratedSummarizer

# Initialize with database connection
db_summarizer = DatabaseIntegratedSummarizer(database, config_path="config.json")

# Summarize artifacts directly from database
result = db_summarizer.summarize_artifact(
    artifact_id=123,
    summary_length="detailed",
    content_type="article"
)

# Summary automatically stored as new artifact
print(f"Summary artifact ID: {result.summary.metadata['database_artifact_id']}")
```

### Database Schema Integration

Summaries are stored as ContextBox artifacts with metadata:

```python
# Summary artifact creation
summary_artifact_id = database.create_artifact(
    capture_id=original_artifact['capture_id'],
    kind='summary',
    title=f"Summary of {original_artifact['title']}",
    text=result.summary.text,
    metadata={
        'original_artifact_id': artifact_id,
        'quality_score': result.quality_metrics['overall'],
        'summary_length': request.summary_length,
        'format_type': request.format_type,
        'llm_provider': result.llm_provider,
        'model_used': result.model_used,
        'processing_time': result.summary.processing_time
    }
)
```

### Retrieval and Search

```python
# Search for summaries by quality score
high_quality_summaries = database.search_artifacts(
    kind='summary',
    metadata_filter={'quality_score': {'$gte': 0.8}}
)

# Get summaries for specific source
source_summaries = database.get_summaries_by_source(source_id)
```

## Error Handling

The system provides comprehensive error handling and recovery:

### Exception Hierarchy

```python
# Base exception
LLMBackendError
â”œâ”€â”€ ConfigurationError
â”œâ”€â”€ AuthenticationError
â”œâ”€â”€ RateLimitError
â”œâ”€â”€ TokenLimitError
â”œâ”€â”€ ModelNotFoundError
â”œâ”€â”€ ServiceUnavailableError
â”œâ”€â”€ ResponseParsingError
â”œâ”€â”€ CostTrackingError
â””â”€â”€ RateLimiterError
```

### Error Handling Examples

```python
try:
    result = manager.summarize_content(request)
    
    # Check for errors
    if result.error:
        print(f"Summarization error: {result.error}")
        return
    
    # Process successful result
    print(f"Summary: {result.summary.text}")
    
except LLMBackendError as e:
    print(f"LLM Backend Error: {e.message}")
    print(f"Provider: {e.provider}")
    print(f"Model: {e.model}")
    print(f"Error Code: {e.error_code}")
    
except ConfigurationError as e:
    print(f"Configuration Error: {e.message}")
    
except ServiceUnavailableError as e:
    print(f"Service Unavailable: {e.message}")
    # Consider fallback to alternative backend
    
except Exception as e:
    print(f"Unexpected error: {e}")
```

### Retry Logic

```python
# Automatic retry with exponential backoff
request = SummaryRequest(
    content=content,
    max_retries=3,
    timeout=30
)

result = manager.summarize_content(request)
```

### Fallback Strategies

```python
# Check available backends
health = manager.health_check()
print(f"System status: {health['overall_status']}")
print(f"Available backends: {list(health['backends'].keys())}")

# Manual backend selection
if 'openai' in manager.backends:
    # Use OpenAI if available
    backend = manager.backends['openai']
elif 'ollama' in manager.backends:
    # Fallback to Ollama
    backend = manager.backends['ollama']
else:
    # Use mock backend for development
    mock_backend = MockLLMBackend()
    backend = mock_backend
```

### Error Recovery

```python
# Quality-based retry
def summarize_with_fallback(content, content_type):
    request = SummaryRequest(
        content=content,
        content_type=content_type,
        quality_threshold=0.8,
        max_retries=2
    )
    
    result = manager.summarize_content(request)
    
    if result.error:
        # Retry with lower quality threshold
        request.quality_threshold = 0.6
        result = manager.summarize_content(request)
        
    if result.error:
        # Retry with brief summary
        request.summary_length = "brief"
        request.quality_threshold = 0.5
        result = manager.summarize_content(request)
    
    return result
```

## Performance Optimization

### Optimization Strategies

#### 1. Cache Management

```python
# Monitor cache performance
stats = manager.get_cache_stats()
cache_hit_rate = stats['total_entries'] / max(1, stats['total_requests'])

# Clear cache periodically
manager.clear_cache(older_than_days=7)  # Weekly cleanup

# Adjust cache size for large deployments
large_manager = SummarizationManager(
    cache_db_path="high_volume_cache.db"
)
```

#### 2. Chunking Optimization

```python
# Optimize chunk size for content type
optimized_chunker = ContentChunker(
    chunk_size=1500,  # Smaller chunks for faster processing
    overlap=150       # Maintain context
)

# Use content-specific chunking
if content_type == "code":
    # Smaller chunks for code
    chunker.chunk_size = 1000
elif content_type == "article":
    # Larger chunks for well-structured articles
    chunker.chunk_size = 2500
```

#### 3. Parallel Processing

```python
# Process multiple documents in parallel
import concurrent.futures

def parallel_summarization(documents):
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        
        for content, content_type in documents:
            request = SummaryRequest(content, content_type)
            future = executor.submit(manager.summarize_content, request)
            futures.append(future)
        
        results = []
        for future in concurrent.futures.as_completed(futures):
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                print(f"Summarization failed: {e}")
                
        return results
```

#### 4. Model Selection

```python
# Use appropriate models for content type
def select_optimal_model(content_type, length_requirement):
    if content_type == "code":
        return "codellama"  # Specialized for code
    elif length_requirement == "brief":
        return "gpt-3.5-turbo"  # Fast for short summaries
    elif length_requirement == "executive":
        return "gpt-4"  # Higher quality for strategic content
    else:
        return "gpt-3.5-turbo"  # Balanced choice

# Configure model per request
model_name = select_optimal_model(content_type, summary_length)
```

#### 5. Batch Processing

```python
# Process similar content in batches
def batch_summarize_articles(articles):
    batch_size = 10
    results = []
    
    for i in range(0, len(articles), batch_size):
        batch = articles[i:i + batch_size]
        
        # Use same model settings for similar content
        batch_requests = [
            SummaryRequest(
                content=article,
                content_type="article",
                summary_length="detailed"
            )
            for article in batch
        ]
        
        # Process batch
        batch_results = [manager.summarize_content(req) for req in batch_requests]
        results.extend(batch_results)
        
    return results
```

### Performance Monitoring

```python
# Monitor processing times
start_time = time.time()
result = manager.summarize_content(request)
processing_time = time.time() - start_time

print(f"Total processing time: {processing_time:.2f}s")
print(f"LLM processing: {result.summary.processing_time:.2f}s")
print(f"Cache hit: {result.cache_hit}")

# Track quality vs performance
quality_performance_ratio = result.quality_metrics['overall'] / processing_time
print(f"Quality per second: {quality_performance_ratio:.2f}")
```

### Resource Management

```python
# Configure memory-efficient processing
import gc

def efficient_summarization(large_documents):
    for doc in large_documents:
        try:
            result = manager.summarize_content(doc)
            process_result(result)
        finally:
            # Explicit cleanup for large documents
            gc.collect()

# Use context managers for resource cleanup
with SummarizationManager() as manager:
    result = manager.summarize_content(request)
    # Automatic cleanup when exiting context
```

### Production Deployment Tips

1. **Use connection pooling** for database operations
2. **Implement circuit breakers** for external API calls
3. **Set appropriate timeouts** based on content size
4. **Monitor memory usage** for large document processing
5. **Use async processing** for high-volume scenarios
6. **Implement health checks** for backend monitoring
7. **Set up alerting** for quality score degradation
8. **Cache model responses** to reduce API costs

---

## Additional Resources

- **Configuration Examples**: See `llm_config.json` template above
- **Testing**: Use MockLLMBackend for development without API costs
- **Monitoring**: Check `health_check()` method for system status
- **Integration**: DatabaseIntegratedSummarizer for seamless ContextBox integration
- **Export**: Multiple formats for different consumption patterns

## Support

For issues and questions:
1. Check error handling section for common solutions
2. Use `health_check()` method to diagnose system issues
3. Enable logging for detailed error information
4. Refer to the exception hierarchy for error types

---

*This documentation covers the complete Intelligent Summarization System. For implementation details, see the source code in `summarization.py` and related modules.*